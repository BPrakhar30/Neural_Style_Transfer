{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Jameschen7/NST_with_self-attention/blob/main/NST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OVqc3AvsQeek"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D6slCEpZofkV",
    "outputId": "8faa2d72-f81b-4c4f-a108-7ca0033063c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "boTfm64TjPb5"
   },
   "source": [
    "## Neural style transfer\n",
    "\n",
    "The implementation of the neural style transfer by [Gatys et al.](https://openaccess.thecvf.com/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) is done using PyTorch library, and follows from the two PyTorch Neural transfer tutorial [here](https://pytorch.org/tutorials/advanced/neural_style_tutorial.html) by Alexis and [here](https://nextjournal.com/gkoehler/pytorch-neural-style-transfer) by Gregor and two other github implementations [here](https://nbviewer.jupyter.org/github/Rabona17/Neural-Style-Tranfer-From-Scratch/blob/master/Neural-Style-Transfer_From_Scratch.ipynb) by Yuan and [here](https://github.com/rpatel26/Neural-Style-Transfer) by Ravi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LglMPh7LT8qP"
   },
   "source": [
    "### Load images and define util functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "06gsfDSuj8SG"
   },
   "outputs": [],
   "source": [
    "# Some util functions\n",
    "unloader = transforms.ToPILImage()  # reconvert into PIL image\n",
    "\n",
    "def load_images(content_path, style_path):\n",
    "    \"\"\" Load and resize content and style so that they have the same size based on the content img\"\"\"\n",
    "    content_img = Image.open(content_path)\n",
    "    style_img = Image.open(style_path)\n",
    "    \n",
    "    imsize = 512 if torch.cuda.is_available() else 128 # use small size if no gpu\n",
    "\n",
    "    content_loader = transforms.Compose([\n",
    "        transforms.Resize(imsize),  # scale imported image, either a PIL Image or a torch Tensor\n",
    "        transforms.ToTensor(),  # transform it into a torch tensor, shape: C x H x W, and convert values between 0 and 255 to be between 0 and 1\n",
    "        ])\n",
    "    content_img = content_loader(content_img)\n",
    "    style_loader = transforms.Compose([\n",
    "        transforms.Resize(content_img.shape[-2:]),  # scale image based on content\n",
    "        transforms.ToTensor()],\n",
    "        ) \n",
    "    style_img = style_loader(style_img)\n",
    "\n",
    "    # fake batch dimension required to fit network's input dimensions\n",
    "    content_img = content_img.unsqueeze(0).to(device, torch.float)\n",
    "    style_img = style_img.unsqueeze(0).to(device, torch.float)\n",
    "    return content_img, style_img\n",
    "\n",
    "def imshow(tensor, title=None, figsize=None):\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)      # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    \n",
    "    plt.ion()\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(image)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001) # pause a bit so that plots are updated in interactive mode\n",
    "\n",
    "def save_image(image, filepath):\n",
    "    \"\"\"\n",
    "    image: tensor\n",
    "    \"\"\"\n",
    "    image = tensor.cpu().clone()  # we clone the tensor to not do changes on it\n",
    "    image = image.squeeze(0)      # remove the fake batch dimension\n",
    "    image = unloader(image)\n",
    "    image.save(filepath, 'jpeg')\n",
    "\n",
    "def tensor_to_numpy(image):\n",
    "    \"\"\"\n",
    "    Convert an image from tensor to ndarray by scaling up by 255 and permuting axis order\n",
    "    \"\"\"\n",
    "    image = image.cpu().clone().squeeze(0).permute(1,2,0)*255\n",
    "    image = image.detach().numpy().astype(np.uint8)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "jco30GG-14tG",
    "outputId": "c0a2cc9c-d947-4d23-a0b7-4dd5ca8caec9"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '\\u202astyle.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11916\\4174510753.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# style_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/picasso.jpg\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mstyle_img_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"â€ªstyle.png\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcontent_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_img_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle_img_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle_img\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle_img\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Style Image'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11916\\1431120854.py\u001b[0m in \u001b[0;36mload_images\u001b[1;34m(content_path, style_path)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;34m\"\"\" Load and resize content and style so that they have the same size based on the content img\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mcontent_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontent_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mstyle_img\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstyle_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mimsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m512\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m128\u001b[0m \u001b[1;31m# use small size if no gpu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\PIL\\Image.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3090\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3091\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3092\u001b[1;33m         \u001b[0mfp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3093\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3094\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '\\u202astyle.png'"
     ]
    }
   ],
   "source": [
    "#content_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/dancing.jpg\"\n",
    "content_img_path = \"dancing.png\"\n",
    "# style_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/picasso.jpg\"\n",
    "style_img_path = \"â€ªstyle.png\"\n",
    "content_img, style_img = load_images(content_img_path, style_img_path)\n",
    "print(content_img.shape, style_img.shape)\n",
    "imshow(style_img, title='Style Image')\n",
    "imshow(content_img, title='Content Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mK4R5z3UDu4"
   },
   "source": [
    "### Content loss\n",
    "\n",
    "The content loss is defined as the weighted sum of mean squared distance between the feature maps of the content image $C^l$ and the generated image $F^l$ at layer $l$, each of size $N_l$ (# distinct filters) times $H_l \\times W_l$ (size):   \n",
    "\n",
    "$$L_{\\text{content}} (C, F, l) = \\frac{1}{2 N_l H_l W_l} \\sum_{i,j}(F^l_{i,j} - C^l_{i,j})^2 $$\n",
    "\n",
    "Besides, according to Gatys' paper, feature maps from lower layers in the network try to capture the exact pixel values of the input image, whereas those from higher layers contains high-level content information in terms of objects and their arrangement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AndHa_KsOGxh"
   },
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target,):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # we 'detach' the target content from the tree used to dynamically compute the gradient: \n",
    "        # this is a stated value, not a variable. Otherwise the forward method of the criterion\n",
    "        # will throw an error.\n",
    "        self.target = target.detach().clone()\n",
    "\n",
    "    def forward(self, input):\n",
    "        loss = F.mse_loss(input, self.target)\n",
    "        # print(\"c\", loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kAPhmdjvfkIm"
   },
   "source": [
    "### Style loss\n",
    "Here the code uses the Gram matrix for style representation. A gram matrix is the result of multiplying a given matrix by its transposed matrix, and here it can capture the correlations between different filter responses at each position over the compared images. The style loss will be a weighed sum of the mean square error between the Gram matrix $G^l \\in \\mathbb{R}^{N_l \\times N_l}$ of the generated image and the Gram matrix $S^l$ of the style image feature maps at layer $l$, each of size $N_l$ (# distinct filters) times $H_l \\times W_l$ (size):   \n",
    "\n",
    "$$ G^l_{i,j} = <F^l_i, F^l_j> $$\n",
    "$$ L_{\\text{style}}(S, F, l) = \\frac{1}{4N_l^2(H_l W_l)^2} \\sum_{i,j} (G^l_{i,j} - S^l_{i,j})^2 $$\n",
    "\n",
    "Based on Gatys, the correlations of feature maps from multiple layers provide a \"stationary, multi-scale representation\" and \"texture information but not the global arrangement\" of the images. Using feature maps up to a higher layer discards more global arrangement of the scene and captures the style at larger scale. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0AWr4wIPdHX3"
   },
   "outputs": [],
   "source": [
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()  # a=batch size(=1); b=# feature maps; (c,d)=dimensions of a f. map\n",
    "    features = input.view(b, c * d)  \n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "    # we 'normalize' the values of the gram matrix by dividing by the number of element in each feature maps.\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach().clone() # the Gram matrix of style image is fixed\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        loss = F.mse_loss(G, self.target)\n",
    "        # print(\"s:\", loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UYkLMPzAofFH"
   },
   "source": [
    "### Attention-based style loss\n",
    "\n",
    "The self-attention module uses the Gaussian function without any embedding space:\n",
    "\n",
    "\\begin{equation}\n",
    "f(x_i,x_j) = e^{x_i^T x_j}\n",
    "\\end{equation}\n",
    "\n",
    "A new style target feature maps $FA$ is created where each position becomes a linear combination of the style image responses $A^l$ with weights based on the dot-product similarity to the content image responses $C^l$ at this location through the softmax function for each interested layer $l$:\n",
    "\n",
    "\\begin{equation}\n",
    "FA^l_{i} = \\frac{1}{\\textbf{C}(FA)} \\sum_{\\forall j} {f(\\overline{C^l_i}, \\overline{A^l_j})A^l_j}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{C}(FA) = \\sum_{\\forall j} {f(\\overline{C^l_i}, \\overline{A^l_j})}\n",
    "\\end{equation}\n",
    "\n",
    "where $i$ denotes a pair of coordinates on the output matrix, and $j$ enumerates all positions on the style image feature map in matrix form, $\\textbf{C}$ represents the softmax normalization factor, and the bar indicates the matrix is mean normalized. Here we do not choose to normalize variance as well like SANet because experiment shows that the synthesized image owns more detail and texture in this way. Comparing the Gram matrix of the new target style feature map $FA$ and the generated image feature map $F$, our attention-based style loss is defined as:\n",
    "\n",
    "\\begin{equation}\n",
    "L_{attention\\_style}(FA, F, l) = L_{style}(SA, G, l)\n",
    "\\end{equation}\n",
    "\n",
    "where $SA$ is the Gram matrix style representation from $FA$, and $G$ is that using $F$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NZdSKHMRs9VE"
   },
   "outputs": [],
   "source": [
    "class AttentionStyleLoss(nn.Module):\n",
    "    def __init__(self, content_feature, style_feature):\n",
    "        super(AttentionStyleLoss, self).__init__()\n",
    "        content_feature = content_feature.detach().clone()\n",
    "        style_feature = style_feature.detach().clone()\n",
    "        N,C,H,W = style_feature.shape\n",
    "        style_feature = style_feature.view(H*W, C)\n",
    "        # print(\"max style_feature:\", torch.max(style_featue))\n",
    "        content_feature = content_feature.view(H*W, C)\n",
    "        # print(\"max content_feature:\", torch.max(content_feature))\n",
    "        # print(\"max softmax 0.5:\", torch.max(torch.mm(style_feature, content_feature)))\n",
    "        softmax = torch.mm(content_feature, style_feature.T)\n",
    "        softmax /= torch.max(softmax)//10\n",
    "        softmax = torch.exp(softmax)\n",
    "        # print(\"nan softmax 1:\", torch.sum(softmax != softmax))\n",
    "        # print(\"max softmax 1:\", torch.sum(softmax))\n",
    "        # print(\"max softmax 1.5:\", torch.max(torch.sum(softmax, dim=1, keepdim=True)), torch.min(torch.sum(softmax, dim=1, keepdim=True)))\n",
    "        softmax = softmax / (torch.sum(softmax, dim=1, keepdim=True) + 1e-8) # add episilon to prevent division by zero\n",
    "        # print(\"nan softmax 2:\", torch.sum(softmax != softmax))\n",
    "        # print(\"max softmax 2:\", torch.max(softmax))\n",
    "        print(softmax.shape, style_feature.shape)\n",
    "        y = torch.mm(softmax, style_feature) # new style feature with weight based on similarity\n",
    "        # print(\"nan softmax 3:\", torch.sum(y != y))\n",
    "        # print(\"max y:\", torch.max(y))\n",
    "        y = y.view(N,C,H,W)\n",
    "        y = (y - torch.mean(y, [2,3], keepdim=True) ) #/torch.std(y, [2,3], keepdim=True)\n",
    "        # self.target = gram_matrix(y)\n",
    "        self.target = y\n",
    "    \n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        # print(input.shape, self.target.shape)\n",
    "        loss = F.mse_loss(G, gram_matrix(self.target))\n",
    "        # loss = F.mse_loss(input, self.target)\n",
    "        # print(loss.item(), G.shape, gram_matrix(self.target).shape, torch.mean(G), torch.mean(gram_matrix(self.target)))\n",
    "        # print(\"s:\", loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnHzQxAbypae"
   },
   "outputs": [],
   "source": [
    "# # v2 the version 2 was used to explore new ideas like comparing features from the second or the six layer using up/downsampling\n",
    "# class AttentionStyleLoss2(nn.Module):\n",
    "#     def __init__(self, content_feature, style_feature, style_feature_use):\n",
    "#         super(AttentionStyleLoss, self).__init__()\n",
    "#         content_feature = content_feature.detach().clone()\n",
    "#         style_feature = style_feature.detach().clone()\n",
    "#         style_feature_use = style_feature_use.detach().clone()\n",
    "#         N,C,H,W = style_feature.shape\n",
    "#         N2,C2,H2,W2 = style_feature_use.shape\n",
    "#         print(H, W, H2, W2)\n",
    "#         print(style_feature_use.shape)\n",
    "#         style_feature = style_feature.view(H*W, C)\n",
    "#         # style_feature_use = style_feature_use.view(H2*W2, C2)\n",
    "#         content_feature = content_feature.view(H*W, C)\n",
    "#         # print(\"max content_feature:\", torch.max(content_feature))\n",
    "#         # print(\"max softmax 0.5:\", torch.max(torch.mm(style_feature, content_feature)))\n",
    "#         softmax = torch.mm(content_feature, style_feature.T)\n",
    "#         softmax /= torch.max(softmax)//10\n",
    "#         softmax = torch.exp(softmax)\n",
    "#         # print(\"nan softmax 1:\", torch.sum(softmax != softmax))\n",
    "#         # print(\"max softmax 1:\", torch.sum(softmax))\n",
    "#         # print(\"max softmax 1.5:\", torch.max(torch.sum(softmax, dim=1, keepdim=True)), torch.min(torch.sum(softmax, dim=1, keepdim=True)))\n",
    "#         softmax = softmax / (torch.sum(softmax, dim=1, keepdim=True) + 1e-8) # add episilon to prevent division by zero\n",
    "#         # print(\"nan softmax 2:\", torch.sum(softmax != softmax))\n",
    "#         # print(\"max softmax 2:\", torch.max(softmax))\n",
    "#         softmax = F.upsample(softmax.unsqueeze(0).unsqueeze(0), scale_factor=1/4, mode=\"nearest\").squeeze()\n",
    "#         style_feature_use = F.upsample(style_feature_use, scale_factor=1/2, mode=\"nearest\").view((H2//2)*(W2//2), C2)\n",
    "#         print(softmax.shape, style_feature_use.shape)\n",
    "#         y = torch.mm(softmax, style_feature_use) # new style feature with weight based on similarity\n",
    "#         # print(\"nan softmax 3:\", torch.sum(y != y))\n",
    "#         # print(\"max y:\", torch.max(y))\n",
    "#         y = y.view(N2,C2,H2//2,W2//2)\n",
    "#         y = y - torch.mean(y, [2,3], keepdim=True) \n",
    "#         # self.target = gram_matrix(y)\n",
    "#         self.target = y\n",
    "    \n",
    "#     def forward(self, input):\n",
    "#         input = F.upsample(input, scale_factor=1/4, mode=\"nearest\")\n",
    "#         G = gram_matrix(input)\n",
    "#         loss = F.mse_loss(G, gram_matrix(self.target))\n",
    "#         # loss = F.mse_loss(input, self.target)\n",
    "#         # print(input.shape, self.target.shape)\n",
    "#         # print(loss.item(), G.shape, gram_matrix(self.target).shape, torch.mean(G), torch.mean(gram_matrix(self.target)))\n",
    "#         # print(\"Attention style forward:\", loss)\n",
    "#         return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyPCDdAEuv-B",
    "outputId": "42fe884b-767b-47db-c019-0f4ff74b31ae"
   },
   "outputs": [],
   "source": [
    "a = torch.tensor([[1,2],[3,4]])\n",
    "torch.sum(a, dim=1, keepdim=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cjv4jdcHWS2q"
   },
   "source": [
    "### Total loss\n",
    "The total loss will just jointly minimize the content loss, the style loss, and the attention-based style loss.\n",
    "\n",
    "$$\n",
    "L_{total}(C, S, FA, F) = \\alpha \\sum_l w^{cl}L_{content}(C,F,l) +\n",
    "\\beta \\sum_l w^{sl} L_{style}(S, F, l) + \\gamma \\sum_l w^{al} L_{style}(FA, F, l)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SzvcEzI0vkyN"
   },
   "outputs": [],
   "source": [
    "# with attention style loss\n",
    "class TotalLoss(nn.Module): \n",
    "    def __init__(self, content_img_features, style_img_features, attention_cont_features, attention_style_features, \n",
    "                 content_layer_weights, style_layer_weights, attention_layer_weights, alpha, beta, attention_weight):\n",
    "        super(TotalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.attention_weight = attention_weight\n",
    "        self.content_layer_weights = content_layer_weights\n",
    "        self.style_layer_weights = style_layer_weights\n",
    "        self.attention_layer_weights = attention_layer_weights\n",
    "\n",
    "        self.content_loss_mods = []\n",
    "        self.style_loss_mods = []\n",
    "        self.attention_style_loss_mods = []\n",
    "        for content_feature in content_img_features.values():\n",
    "            self.content_loss_mods.append(ContentLoss(content_feature))\n",
    "        for style_feature in style_img_features.values():\n",
    "            self.style_loss_mods.append(StyleLoss(style_feature))\n",
    "        for attention_cont_feature, attention_style_feature in zip(attention_cont_features.values(), attention_style_features.values()):\n",
    "            self.attention_style_loss_mods.append(AttentionStyleLoss(attention_cont_feature, attention_style_feature))\n",
    "        print(f\"# content layer: {len(content_img_features)}, # style layer: {len(style_img_features)}, # attention layer: {len(attention_cont_features)}\")\n",
    "\n",
    "    def forward(self, content_features, style_features, attention_features):\n",
    "        content_loss, style_loss = 0, 0\n",
    "        attention_style_loss = 0\n",
    "        for feature, c_loss_mod, layer_weight in zip(content_features.values(), self.content_loss_mods, self.content_layer_weights):\n",
    "            content_loss += layer_weight * c_loss_mod(feature)\n",
    "        for feature, s_loss_mod, layer_weight in zip(style_features.values(), self.style_loss_mods, self.style_layer_weights):\n",
    "            # display(feature.shape, layer_weight, s_loss_mod(feature))\n",
    "            style_loss += layer_weight * s_loss_mod(feature) \n",
    "        for feature, a_loss_mod, layer_weight in zip(attention_features.values(), self.attention_style_loss_mods, self.attention_layer_weights):\n",
    "            # print(len(self.attention_style_loss_mods), len(attention_features.values()), feature.shape, a_loss_mod(feature).item())\n",
    "            attention_style_loss += layer_weight * a_loss_mod(feature) \n",
    "\n",
    "        total_loss = self.alpha * content_loss + self.beta * style_loss + self.attention_weight * attention_style_loss \n",
    "        # print((self.alpha * content_loss).item(), (self.beta * style_loss).item())\n",
    "        self.content_score = (self.alpha * content_loss).item()\n",
    "        self.style_score = (self.beta * style_loss).item()\n",
    "        self.attention_style_score = (self.attention_weight * attention_style_loss)\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9BPkncRacNK"
   },
   "outputs": [],
   "source": [
    "# # with attention style loss 2\n",
    "# class TotalLoss(nn.Module): \n",
    "#     def __init__(self, content_img_features, style_img_features, attention_cont_cont_features, attention_style_cont_features, attention_style_style_features,\n",
    "#                  content_layer_weights, style_layer_weights, attention_layer_weights, alpha, beta, attention_weight):\n",
    "#         super(TotalLoss, self).__init__()\n",
    "#         self.alpha = alpha\n",
    "#         self.beta = beta\n",
    "#         self.attention_weight = attention_weight\n",
    "#         self.content_layer_weights = content_layer_weights\n",
    "#         self.style_layer_weights = style_layer_weights\n",
    "#         self.attention_layer_weights = attention_layer_weights\n",
    "\n",
    "#         self.content_loss_mods = []\n",
    "#         self.style_loss_mods = []\n",
    "#         self.attention_style_loss_mods = []\n",
    "#         for content_feature in content_img_features.values():\n",
    "#             self.content_loss_mods.append(ContentLoss(content_feature))\n",
    "#         for style_feature in style_img_features.values():\n",
    "#             self.style_loss_mods.append(StyleLoss(style_feature))\n",
    "#         for attention_cont_feature, attention_style_feature, attention_ss_feature in zip(attention_cont_cont_features.values(), \n",
    "#                                                                    attention_style_cont_features.values(), attention_style_style_features.values()):\n",
    "#             print(\"in total init:\",attention_ss_feature.shape)\n",
    "#             self.attention_style_loss_mods.append(AttentionStyleLoss(attention_cont_feature, attention_style_feature, attention_ss_feature))\n",
    "#         # print(f\"# content layer: {len(content_img_features)}, # style layer: {len(style_img_features)}, # attention layer: {len(attention_cont_features)}\")\n",
    "#         print(f\"# content layer: {len(content_img_features)}, # style layer: {len(style_img_features)}, # attention layer: {len(attention_style_style_features)}\")\n",
    "\n",
    "#     def forward(self, content_features, style_features, attention_features, attention_use_features):\n",
    "#         content_loss, style_loss = 0, 0\n",
    "#         attention_style_loss = 0\n",
    "#         for feature, c_loss_mod, layer_weight in zip(content_features.values(), self.content_loss_mods, self.content_layer_weights):\n",
    "#             content_loss += layer_weight * c_loss_mod(feature)\n",
    "#         for feature, s_loss_mod, layer_weight in zip(style_features.values(), self.style_loss_mods, self.style_layer_weights):\n",
    "#             # display(feature.shape, layer_weight, s_loss_mod(feature))\n",
    "#             style_loss += layer_weight * s_loss_mod(feature) \n",
    "#         for feature, a_loss_mod, layer_weight in zip(attention_use_features.values(), self.attention_style_loss_mods, self.attention_layer_weights):\n",
    "#             # print(len(self.attention_style_loss_mods), len(attention_features.values()), feature.shape, a_loss_mod(feature).item())\n",
    "#             # print(\"in for loop:\", a_loss_mod(feature) , layer_weight)\n",
    "#             attention_style_loss += layer_weight * a_loss_mod(feature) \n",
    "\n",
    "#         total_loss = self.alpha * content_loss + self.beta * style_loss + self.attention_weight * attention_style_loss \n",
    "#         self.content_score = (self.alpha * content_loss).item()\n",
    "#         self.style_score = (self.beta * style_loss).item()\n",
    "#         self.attention_style_score = (self.attention_weight * attention_style_loss)\n",
    "#         # print((self.alpha * content_loss).item(), (self.beta * style_loss).item(), self.attention_weight * attention_style_loss)\n",
    "#         return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tWnN3EGnv1eS"
   },
   "source": [
    "### Deep Convolutional Neural Networks\n",
    "A pre-trained VGG-19 network like the one used in Gatys' paper can effectively learn the low-level and high-level semantic features in different images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DZjNjXojuV2-"
   },
   "outputs": [],
   "source": [
    "# PyTorchâ€™s implementation of VGG is a module divided into two child Sequential modules: \n",
    "# features (containing convolution and pooling layers), \n",
    "# and classifier (containing fully connected layers). \n",
    "cnn = models.vgg19(pretrained=True).features.to(device).eval() # only need features part \n",
    "for param in cnn.parameters():\n",
    "    param.requires_grad_(False)\n",
    "\n",
    "# a trick suggested by Gregor as it's used by Gatys\n",
    "for i, layer in enumerate(cnn):\n",
    "    if isinstance(layer, torch.nn.MaxPool2d):\n",
    "        cnn[i] = torch.nn.AvgPool2d(kernel_size=2, stride=2, padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "627cwBpu2Y5l",
    "outputId": "512933fe-2e1b-4614-df89-2e78f03bd190"
   },
   "outputs": [],
   "source": [
    "list(cnn.named_children())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rvqpr2q_Qhu5"
   },
   "source": [
    "Extract feature maps by manually going through the model and save the wanted layer after mean normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GuzBPOXsM5ZG",
    "outputId": "7dcb1102-eb9e-4514-b3bf-7bd78b8c1a91"
   },
   "outputs": [],
   "source": [
    "# Additionally, VGG networks are trained on images with each channel normalized by \n",
    "# mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]. \n",
    "# We will use them to normalize the image before sending it into the network.\n",
    "cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self, mean, std):\n",
    "        super(Normalization, self).__init__()\n",
    "        self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "        self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "normalization = Normalization(cnn_normalization_mean, cnn_normalization_std).to(device)\n",
    "\n",
    "def get_features(image, cnn, content_layer:\"dict\", style_layer:\"dict\", attention_style_layer):\n",
    "    content_features = {}\n",
    "    style_features = {}\n",
    "    attention_features = {}\n",
    "    x = normalization(image)\n",
    "    for name, layer in enumerate(cnn):\n",
    "        x = layer(x)\n",
    "        if name in [0, 5, 10, 19, 28]:\n",
    "        # if isinstance(layer, nn.Conv2d):\n",
    "            x = x - torch.mean(x, [2,3], keepdim=True) \n",
    "        if str(name) in content_layer:\n",
    "            # content_features[content_layer[str(name)]] = x.clone()\n",
    "            feature_map = x.clone()\n",
    "            # feature_map = feature_map - torch.mean(feature_map, [2,3], keepdim=True) \n",
    "            # x = feature_map\n",
    "            content_features[content_layer[str(name)]] = feature_map\n",
    "        if str(name) in style_layer:\n",
    "            # style_features[style_layer[str(name)]] = x.clone()\n",
    "            feature_map = x.clone()\n",
    "            # feature_map = feature_map - torch.mean(feature_map, [2,3], keepdim=True) \n",
    "            # x = feature_map\n",
    "            style_features[style_layer[str(name)]] = feature_map\n",
    "        if str(name) in attention_style_layer:\n",
    "            feature_map = x.clone()\n",
    "            # normalize\n",
    "            feature_map = (feature_map - torch.mean(feature_map, [2,3], keepdim=True)) / (torch.std(feature_map, [2,3], keepdim=True) + 1e-10)\n",
    "            # feature_map = F.instance_norm(feature_map)\n",
    "            # print(\"in get feature_map:\", torch.max(feature_map))\n",
    "            attention_features[attention_style_layer[str(name)]] = feature_map\n",
    "    return content_features, style_features, attention_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i9qCsC9zDx8d"
   },
   "outputs": [],
   "source": [
    "# # Additionally, VGG networks are trained on images with each channel normalized by \n",
    "# # mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]. \n",
    "# # We will use them to normalize the image before sending it into the network.\n",
    "# cnn_normalization_mean = torch.tensor([0.485, 0.456, 0.406]).to(device)\n",
    "# cnn_normalization_std = torch.tensor([0.229, 0.224, 0.225]).to(device)\n",
    "\n",
    "# class Normalization(nn.Module):\n",
    "#     def __init__(self, mean, std):\n",
    "#         super(Normalization, self).__init__()\n",
    "#         self.mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "#         self.std = torch.tensor(std).view(-1, 1, 1)\n",
    "\n",
    "#     def forward(self, img):\n",
    "#         return (img - self.mean) / self.std\n",
    "\n",
    "# normalization = Normalization(cnn_normalization_mean, cnn_normalization_std).to(device)\n",
    "\n",
    "# def get_features(image, cnn, content_layer:\"dict\", style_layer:\"dict\", attention_style_layer, attention_style_use_layer):\n",
    "#     content_features = {}\n",
    "#     style_features = {}\n",
    "#     attention_features = {}\n",
    "#     attention_style_use_features = {}\n",
    "#     x = normalization(image)\n",
    "#     for name, layer in enumerate(cnn):\n",
    "#         x = layer(x)\n",
    "#         if name in [0, 5, 10, 19, 28]:\n",
    "#         # if isinstance(layer, nn.Conv2d):\n",
    "#             x = x - torch.mean(x, [2,3], keepdim=True) \n",
    "#             # x = F.instance_norm(x)\n",
    "#         if str(name) in content_layer:\n",
    "#             # content_features[content_layer[str(name)]] = x.clone()\n",
    "#             feature_map = x.clone()\n",
    "#             # feature_map = feature_map - torch.mean(feature_map, [2,3], keepdim=True) \n",
    "#             # x = feature_map\n",
    "#             content_features[content_layer[str(name)]] = feature_map\n",
    "#         if str(name) in style_layer:\n",
    "#             # style_features[style_layer[str(name)]] = x.clone()\n",
    "#             feature_map = x.clone()\n",
    "#             # feature_map = feature_map - torch.mean(feature_map, [2,3], keepdim=True) \n",
    "#             # x = feature_map\n",
    "#             style_features[style_layer[str(name)]] = feature_map\n",
    "#         if str(name) in attention_style_layer:\n",
    "#             feature_map = x.clone()\n",
    "#             # feature_map = (feature_map - torch.mean(feature_map, [2,3], keepdim=True)) / (torch.std(feature_map, [2,3], keepdim=True) + 1e-10)\n",
    "#             # print(\"in get feature_map:\", torch.max(feature_map))\n",
    "#             attention_features[attention_style_layer[str(name)]] = feature_map\n",
    "#         if str(name) in attention_style_use_layer:\n",
    "#             feature_map = x.clone()\n",
    "#             feature_map = (feature_map - torch.mean(feature_map, [2,3], keepdim=True)) # / (torch.std(feature_map, [2,3], keepdim=True) + 1e-10)\n",
    "#             # print(\"in get feature_map:\", torch.max(feature_map))\n",
    "#             attention_style_use_features[attention_style_use_layer[str(name)]] = feature_map\n",
    "#     return content_features, style_features, attention_features, attention_style_use_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAc1iE8u5OlN"
   },
   "source": [
    "### Model\n",
    "Combine everything together and define a function to train the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "7Urd_IR_5sLh",
    "outputId": "8d5670b6-2171-4398-884c-596b263c7ac4"
   },
   "outputs": [],
   "source": [
    "# Initialize an image by adding some white noise onto the original content image\n",
    "def initialize_img(content_img, noise_ratio=0.6):\n",
    "    input_img = content_img.clone()\n",
    "    noise_img = torch.randn(content_img.data.size(), device=device)\n",
    "    noise_ratio = noise_ratio\n",
    "    input_img = noise_ratio * noise_img + (1 - noise_ratio) * input_img\n",
    "    input_img.data.clamp_(0, 1)\n",
    "    return input_img.requires_grad_()\n",
    "\n",
    "input_img = initialize_img(content_img)\n",
    "# add the original input image to the figure:\n",
    "plt.figure()\n",
    "imshow(input_img, title='Input Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LodKwlWa5Zws"
   },
   "source": [
    "Use L-BFGS algorithm to run our gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ELud-1Gr5qTC"
   },
   "outputs": [],
   "source": [
    "def get_input_optimizer(input_img):\n",
    "    # this line to show that input is a parameter that requires a gradient\n",
    "    optimizer = optim.LBFGS([input_img.requires_grad_()])\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2PbkDtGSCZyS"
   },
   "outputs": [],
   "source": [
    "#v2\n",
    "# def run_style_transfer(cnn, content_img, style_img, num_steps=300, noise_ratio=0.6,\n",
    "#                        content_weight=1, style_weight=1000000, attention_weight=1e4,\n",
    "#                        content_layer=None, style_layer=None, attention_style_layer=None, attention_style_use_layer=None,\n",
    "#                        content_layer_weights=None, style_layer_weights=None, attention_layer_weights=None):\n",
    "#     print('Building the style transfer model..')\n",
    "#     if content_layer is None:  # provide the default layers used by Gatys' paper,\n",
    "#         content_layer = {     \n",
    "#             '21': 'conv4_2',  \n",
    "#             }\n",
    "#     if style_layer is None: \n",
    "#         style_layer = {\n",
    "#             '0': 'conv1_1',\n",
    "#             '5': 'conv2_1',\n",
    "#             '10': 'conv3_1',\n",
    "#             '19': 'conv4_1',\n",
    "#             '28': 'conv5_1'\n",
    "#             }\n",
    "#     if attention_style_layer is None: \n",
    "#         attention_style_layer={\n",
    "#             '19': 'conv4_1',\n",
    "#             # '28': 'conv5_1'\n",
    "#         }\n",
    "#     # default equal weight\n",
    "#     if content_layer_weights is None:\n",
    "#         content_layer_weights = np.full(len(content_layer), 1/len(content_layer))\n",
    "#     if style_layer_weights is None:\n",
    "#         style_layer_weights = np.full(len(style_layer), 1/len(style_layer))\n",
    "#     if attention_layer_weights is None:\n",
    "#         attention_layer_weights = np.full(len(attention_style_layer), 1/len(attention_style_layer))\n",
    "#     print(content_layer_weights, style_layer_weights, attention_layer_weights, attention_style_use_layer)\n",
    "\n",
    "#     # now we trim off the layers after the last content and style losses\n",
    "#     cnn = copy.deepcopy(cnn)\n",
    "#     for i in range(len(cnn) - 1, -1, -1):\n",
    "#         if str(i) in content_layer or str(i) in style_layer or str(i) in attention_style_layer or str(i) in attention_style_use_layer:\n",
    "#             break\n",
    "#     cnn = cnn[:(i + 1)]\n",
    "\n",
    "#     # calculate the features for the content and style images and set up the loss module\n",
    "#     content_features, _,  content_attention_features, _ = get_features(content_img, cnn, content_layer, style_layer, attention_style_layer, attention_style_use_layer)\n",
    "#     style_content_features, style_features, style_attention_features, attention_style_use_features = get_features(style_img, cnn, content_layer, style_layer, attention_style_layer, attention_style_use_layer)\n",
    "#     total_loss = TotalLoss(content_features, style_features, content_attention_features, style_attention_features, attention_style_use_features,\n",
    "#                            content_layer_weights, style_layer_weights, attention_layer_weights,\n",
    "#                             content_weight, style_weight, attention_weight)\n",
    "#     # total_loss = TotalLoss(content_features, style_features, content_attention_features, style_attention_features, \n",
    "#     #                        content_layer_weights, style_layer_weights, attention_layer_weights,\n",
    "#     #                         content_weight, style_weight, attention_weight)\n",
    "\n",
    "#     # Optimize the image\n",
    "#     input_img = initialize_img(content_img, noise_ratio=noise_ratio)\n",
    "#     optimizer = get_input_optimizer(input_img) # LBFGS seems to be faster\n",
    "#     # optimizer = optim.Adam([input_img], lr=0.01)\n",
    "\n",
    "#     start = time.time()\n",
    "#     run = [-1] # in a list so that the LBFGS can modify and keep it\n",
    "#     while run[0] <= num_steps+1:\n",
    "#         def closure(): # define a closure method for the \n",
    "#             input_img.data.clamp_(0, 1)\n",
    "#             optimizer.zero_grad()\n",
    "#             input_content_features, input_style_features, input_attention_features, input_attention_use_features = get_features(input_img, cnn, content_layer, style_layer, attention_style_layer, attention_style_use_layer)\n",
    "#             loss = total_loss(input_content_features, input_style_features, input_attention_features, input_attention_use_features)\n",
    "#             # calculate the Total Variation Denoising term (minimize the different if shift 1 px by x or y)\n",
    "#             # denoise = 0\n",
    "#             denoise =  torch.mean((input_img[:, :, 1:, :] - input_img[:,:, :-1,:]).abs()) + \\\n",
    "#                             torch.mean((input_img[:, :, :, 1:] - input_img[:,:, :, :-1]).abs())\n",
    "#             loss += denoise/2\n",
    "#             loss.backward() \n",
    "\n",
    "#             run[0] += 1\n",
    "#             if run[0] % 50 == 0:\n",
    "#                 print(f\"Iteration {run}, Total loss: {loss.item()}\")\n",
    "#                 print('Style Loss : {:4f} Content Loss: {:4f} Noise Loss: {:.4f} Attenstion score: {:.4f}'.format(\n",
    "#                     total_loss.style_score, total_loss.content_score, denoise, total_loss.attention_style_score))\n",
    "#             return loss\n",
    "\n",
    "#         optimizer.step(closure)\n",
    "#     input_img.data.clamp_(0, 1)\n",
    "#     print(f\"Total time: {time.time() - start}\")\n",
    "#     return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u-mJgZxhiM_1"
   },
   "outputs": [],
   "source": [
    "def run_style_transfer(cnn, content_img, style_img, num_steps=300, noise_ratio=0.6,\n",
    "                       content_weight=1, style_weight=1000000, attention_weight=1e4,\n",
    "                       content_layer=None, style_layer=None, attention_style_layer=None,\n",
    "                       content_layer_weights=None, style_layer_weights=None, attention_layer_weights=None):\n",
    "    print('Building the style transfer model..')\n",
    "    if content_layer is None:  # provide the default layers used by Gatys' paper,\n",
    "        content_layer = {     \n",
    "            '21': 'conv4_2',  \n",
    "            }\n",
    "    if style_layer is None: \n",
    "        style_layer = {\n",
    "            '0': 'conv1_1',\n",
    "            '5': 'conv2_1',\n",
    "            '10': 'conv3_1',\n",
    "            '19': 'conv4_1',\n",
    "            '28': 'conv5_1'\n",
    "            }\n",
    "    if attention_style_layer is None: \n",
    "        attention_style_layer={\n",
    "            '19': 'conv4_1',\n",
    "            # '28': 'conv5_1'\n",
    "        }\n",
    "    # default equal weight\n",
    "    if content_layer_weights is None:\n",
    "        content_layer_weights = np.full(len(content_layer), 1/len(content_layer))\n",
    "    if style_layer_weights is None:\n",
    "        style_layer_weights = np.full(len(style_layer), 1/len(style_layer))\n",
    "    if attention_layer_weights is None:\n",
    "        attention_layer_weights = np.full(len(attention_style_layer), 1/len(attention_style_layer))\n",
    "    print(content_layer_weights, style_layer_weights, attention_layer_weights)\n",
    "\n",
    "    # now we trim off the layers after the last content and style losses\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "    for i in range(len(cnn) - 1, -1, -1):\n",
    "        if str(i) in content_layer or str(i) in style_layer or str(i) in attention_style_layer:\n",
    "            break\n",
    "    cnn = cnn[:(i + 1)]\n",
    "\n",
    "    # calculate the features for the content and style images and set up the loss module\n",
    "    \n",
    "    content_features, _,  content_attention_features = get_features(content_img, cnn, content_layer, style_layer, attention_style_layer)\n",
    "    style_content_features, style_features, style_attention_features = get_features(style_img, cnn, content_layer, style_layer, attention_style_layer)\n",
    "    # total_loss = TotalLoss(content_features, style_features, content_attention_features, style_attention_features, style_attention_features,\n",
    "    #                        content_layer_weights, style_layer_weights, attention_layer_weights,\n",
    "    #                         content_weight, style_weight, attention_weight)\n",
    "    total_loss = TotalLoss(content_features, style_features, content_attention_features, style_attention_features, \n",
    "                           content_layer_weights, style_layer_weights, attention_layer_weights,\n",
    "                            content_weight, style_weight, attention_weight)\n",
    "\n",
    "    # Optimize the image\n",
    "    input_img = initialize_img(content_img, noise_ratio=noise_ratio)\n",
    "    optimizer = get_input_optimizer(input_img) # LBFGS seems to be faster\n",
    "    # optimizer = optim.Adam([input_img], lr=0.01)\n",
    "\n",
    "    start = time.time()\n",
    "    run = [-1] # in a list so that the LBFGS can modify and keep it\n",
    "    while run[0] <= num_steps+1:\n",
    "        def closure(): # define a closure method for the \n",
    "            input_img.data.clamp_(0, 1)\n",
    "            optimizer.zero_grad()\n",
    "            input_content_features, input_style_features, input_attention_features = get_features(input_img, cnn, content_layer, style_layer, attention_style_layer)\n",
    "            loss = total_loss(input_content_features, input_style_features, input_attention_features)\n",
    "            # calculate the Total Variation Denoising term (minimize the different if shift 1 px by x or y)\n",
    "            # denoise = 0\n",
    "            denoise =  torch.mean((input_img[:, :, 1:, :] - input_img[:,:, :-1,:]).abs()) + \\\n",
    "                            torch.mean((input_img[:, :, :, 1:] - input_img[:,:, :, :-1]).abs())\n",
    "            loss += denoise/2\n",
    "            loss.backward() \n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(f\"Iteration {run}, Total loss: {loss.item()}\")\n",
    "                print('Style Loss : {:4f} Content Loss: {:4f} Noise Loss: {:.4f} Attenstion score: {:.4f}'.format(\n",
    "                    total_loss.style_score, total_loss.content_score, denoise, total_loss.attention_style_score))\n",
    "            return loss\n",
    "\n",
    "        optimizer.step(closure)\n",
    "    input_img.data.clamp_(0, 1)\n",
    "    end = time.time() - start\n",
    "    print(f\"Total time: {end}\")\n",
    "    return input_img, end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AU3-hwLz1_EI"
   },
   "source": [
    "### Generate new image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fNYKF-6imHqF",
    "outputId": "65840eeb-28cd-480f-fc76-75ba92229ca6"
   },
   "outputs": [],
   "source": [
    "content_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/Tuebingen_Neckarfront.jpg\"\n",
    "style_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/Starry_Night.jpg\"\n",
    "# content_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/dancing.jpg\"\n",
    "# style_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/picasso.jpg\"\n",
    "# style_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/Tuebingen_Neckarfront.jpg\"\n",
    "# content_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/building.jpg\"\n",
    "# content_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/women2.jpg\"\n",
    "# style_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/wave2.jpg\"\n",
    "# style_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/brushstrokes.jpg\"\n",
    "content_img, style_img = load_images(content_img_path, style_img_path)\n",
    "\n",
    "# hyperparameters\n",
    "content_layer = {\n",
    "    # \"19\": \"conv4_1\",\n",
    "    # \"28\": \"conv5_1\",\n",
    "    '21': 'conv4_2',  \n",
    "    }\n",
    "style_layer = {\n",
    "    '0': 'conv1_1',\n",
    "    '5': 'conv2_1',\n",
    "    '10': 'conv3_1',\n",
    "    '19': 'conv4_1',\n",
    "    '28': 'conv5_1'\n",
    "    }\n",
    "attention_style_layer={\n",
    "    # '10': 'conv3_1',\n",
    "    # '12': 'conv3_2',\n",
    "    # '16': 'conv3_4',\n",
    "    '19': 'conv4_1',\n",
    "    # '21': 'conv4_2', \n",
    "    # '24': 'conv4_3', \n",
    "    # '25': 'conv4_4',\n",
    "    # '5': 'conv2_1', # no enough memory for softmax\n",
    "    # '28': 'conv5_1',\n",
    "    # '30': 'conv5_2',\n",
    "}\n",
    "# attention_style_use_layer={\n",
    "#     '28': 'conv5_1',\n",
    "# }\n",
    "# 5 7e7\n",
    "content_weight=1\n",
    "style_weight=1e6\n",
    "attention_weight=5e6\n",
    "content_layer_weights = None\n",
    "style_layer_weights=[0.2, 0.2, 0.2, 0.2, 0.2]\n",
    "attention_layer_weights = None\n",
    "\n",
    "output, runtime = run_style_transfer(cnn, content_img, style_img, num_steps=600, noise_ratio=1, \n",
    "                       content_weight=content_weight, style_weight=style_weight, attention_weight=attention_weight,\n",
    "                       content_layer=content_layer, style_layer=style_layer, attention_style_layer=attention_style_layer,\n",
    "                       content_layer_weights=content_layer_weights, style_layer_weights=style_layer_weights,\n",
    "                       attention_layer_weights=attention_layer_weights)\n",
    "# output = run_style_transfer(cnn, content_img, style_img, num_steps=400, noise_ratio=1, \n",
    "#                        content_weight=content_weight, style_weight=style_weight, attention_weight=attention_weight,\n",
    "#                        content_layer=content_layer, style_layer=style_layer, attention_style_layer=attention_style_layer, attention_style_use_layer=attention_style_use_layer,\n",
    "#                        content_layer_weights=content_layer_weights, style_layer_weights=style_layer_weights,\n",
    "#                        attention_layer_weights=attention_layer_weights)\n",
    "\n",
    "# print(content_layer, style_layer, attention_style_layer, content_weight, style_weight, attention_weight)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 9))\n",
    "ax[0, 0].imshow(tensor_to_numpy(content_img))\n",
    "ax[0, 0].axis(False)\n",
    "ax[0, 0].set_title(\"Content Image\")\n",
    "ax[0, 1].imshow(tensor_to_numpy(style_img))\n",
    "ax[0, 1].axis(False)\n",
    "ax[0, 1].set_title(\"Style Image\")\n",
    "ax[1, 0].imshow(tensor_to_numpy(output))\n",
    "ax[1, 0].axis(False)\n",
    "ax[1, 0].set_title(\"LBFGS\")\n",
    "ax[1, 1].imshow(tensor_to_numpy(prev))\n",
    "ax[1, 1].axis(False)\n",
    "ax[1, 1].set_title(\"Adam\")\n",
    "plt.tight_layout()\n",
    "runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 762
    },
    "id": "9Yb0uowY2li2",
    "outputId": "116a2da9-51ee-44e3-a7a4-ff45d52d1803"
   },
   "outputs": [],
   "source": [
    "print(\"no att\")\n",
    "print(content_layer, style_layer, attention_style_layer, content_weight, style_weight, attention_weight,  )\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 7))\n",
    "ax[0].imshow(tensor_to_numpy(content_img))\n",
    "ax[1].imshow(tensor_to_numpy(style_img))\n",
    "imshow(output, title='Output Image', figsize=(10,10));\n",
    "prev = output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "id": "2c_9DMc0zmlE",
    "outputId": "59158ee5-43cf-40d9-a0ef-eb5dda0565e0"
   },
   "outputs": [],
   "source": [
    "print(content_layer, style_layer, attention_style_layer, content_weight, style_weight, attention_weight,  )\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(tensor_to_numpy(output))\n",
    "plt.axis(False)\n",
    "# ax[1, 0].set_title(\"Ours\")\n",
    "# ax[1, 1].imshow(tensor_to_numpy(prev))\n",
    "# ax[1, 1].axis(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r57AbI4oaKbf"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "# print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 745
    },
    "id": "FPR8wIxKtpT3",
    "outputId": "4c74ad3e-5afe-4039-cb4d-fd6f863caf5f"
   },
   "outputs": [],
   "source": [
    "print(content_layer, style_layer, attention_style_layer, content_weight, style_weight, attention_weight)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 7))\n",
    "ax[0].imshow(tensor_to_numpy(content_img))\n",
    "ax[1].imshow(tensor_to_numpy(style_img))\n",
    "imshow(output, title='Output Image', figsize=(10,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5XJOBPLSzEor"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YhDKwnGTqdZc"
   },
   "source": [
    "### Result of comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 783
    },
    "id": "bW5c8X_tqdBr",
    "outputId": "7c7e8516-bcab-4f83-dd09-9b13f1916311"
   },
   "outputs": [],
   "source": [
    "print(content_layer, style_layer, attention_style_layer, content_weight, style_weight, attention_weight)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(10, 10))\n",
    "ax[0, 0].imshow(tensor_to_numpy(content_img))\n",
    "ax[0, 0].axis(False)\n",
    "ax[0, 0].set_title(\"Content Image\")\n",
    "ax[0, 1].imshow(tensor_to_numpy(style_img))\n",
    "ax[0, 1].axis(False)\n",
    "ax[0, 1].set_title(\"Style Image\")\n",
    "ax[1, 0].imshow(tensor_to_numpy(output))\n",
    "ax[1, 0].axis(False)\n",
    "ax[1, 0].set_title(\"Ours\")\n",
    "ax[1, 1].imshow(tensor_to_numpy(prev))\n",
    "ax[1, 1].axis(False)\n",
    "ax[1, 1].set_title(\"Vanilla\")\n",
    "plt.tight_layout()\n",
    "runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ANxL6pskKlTN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1q9jBfWlv2yP"
   },
   "source": [
    "### Visualize weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0U28HttwJmg"
   },
   "outputs": [],
   "source": [
    "content_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/Tuebingen_Neckarfront.jpg\"\n",
    "# style_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/Starry_Night.jpg\"\n",
    "# content_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/dancing.jpg\"\n",
    "style_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/picasso.jpg\"\n",
    "content_img, style_img = load_images(content_img_path, style_img_path)\n",
    "\n",
    "content_layer = {\n",
    "    # \"19\": \"conv4_1\",\n",
    "    # \"28\": \"conv5_1\",\n",
    "    '21': 'conv4_2',  \n",
    "    }\n",
    "style_layer = {\n",
    "    '0': 'conv1_1',\n",
    "    '5': 'conv2_1',\n",
    "    '10': 'conv3_1',\n",
    "    '19': 'conv4_1',\n",
    "    '28': 'conv5_1'\n",
    "    }\n",
    "attention_style_layer={\n",
    "    # '19': 'conv4_1',\n",
    "    # '21': 'conv4_2', \n",
    "    # '28': 'conv5_1',\n",
    "    '10': 'conv3_1',\n",
    "}\n",
    "content_features, _,  content_attention_features = get_features(content_img, cnn, content_layer, style_layer, attention_style_layer)\n",
    "_, style_features, style_attention_features = get_features(style_img, cnn, content_layer, style_layer, attention_style_layer)\n",
    "content_attention_features = content_attention_features[\"conv3_1\"]\n",
    "style_attention_features = style_attention_features[\"conv3_1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QjDg8eMiv4ZP"
   },
   "outputs": [],
   "source": [
    "content_feature = content_attention_features.detach().clone()\n",
    "style_feature = style_attention_features.detach().clone()\n",
    "N,C,H,W = style_feature.shape\n",
    "# style_feature = style_feature.view(H*W, C)\n",
    "# content_feature = content_feature.view(C, H*W)\n",
    "# softmax = torch.mm(style_feature, content_feature)\n",
    "# softmax /= torch.max(softmax)//10\n",
    "# softmax = torch.exp(softmax)\n",
    "\n",
    "style_feature = style_feature.view(H*W, C)\n",
    "content_feature = content_feature.view(H*W, C)\n",
    "softmax = torch.mm(content_feature, style_feature.T)\n",
    "softmax /= torch.max(softmax)//10\n",
    "softmax = torch.exp(softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "05CUgEebwoaf",
    "outputId": "5199ff68-d651-4c6e-f6ba-07f930d26db0"
   },
   "outputs": [],
   "source": [
    "style_feature.shape, softmax.shape, H,W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vjbmh3QTrPEv",
    "outputId": "760736ff-e36c-40fd-e6bd-5ca88e5f4b3d"
   },
   "outputs": [],
   "source": [
    "# softmax = softmax.squeeze()\n",
    "factor = 4\n",
    "x_pos, y_pos = 14, 35\n",
    "\n",
    "pos = x_pos*W + y_pos\n",
    "# softmax = F.interpolate(softmax.unsqueeze(0).unsqueeze(0), scale_factor=1/4).squeeze()\n",
    "highest = softmax[pos].argmax()\n",
    "# x_pos2, y_pos2 = highest//(W//2), highest%(W//2)\n",
    "x_pos2, y_pos2 = highest//(W), highest%(W)\n",
    "x_pos2, y_pos2 = x_pos2.cpu().item(), y_pos2.cpu().item()\n",
    "\n",
    "x_pos, y_pos = x_pos*factor, y_pos*factor\n",
    "x_pos3, y_pos3 = x_pos2*factor, y_pos2*factor\n",
    "print(softmax.shape)\n",
    "row = softmax[pos].view(H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CPYHPggnKc6L",
    "outputId": "37c93c42-745a-46d3-fc47-2b28c6c2d721"
   },
   "outputs": [],
   "source": [
    "x_pos, y_pos, pos, highest, x_pos2, y_pos2, row.max(), row[x_pos2, y_pos2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 233
    },
    "id": "3Apy_iFZslKX",
    "outputId": "d6098507-32d7-4324-c140-a207bb26a217"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 3, figsize=(15, 10))\n",
    "ax[0].imshow(tensor_to_numpy(content_img))\n",
    "ax[0].plot(y_pos, x_pos, \"ro\")\n",
    "ax[0].axis(False)\n",
    "\n",
    "ax[1].imshow(tensor_to_numpy(style_img))\n",
    "ax[1].plot(y_pos3, x_pos3, \"ro\")\n",
    "ax[1].axis(False)\n",
    "ax[1].axis(False)\n",
    "ax[2].imshow(row.cpu().squeeze(), \"gray\")\n",
    "ax[2].plot(y_pos2, x_pos2, \"r.\")\n",
    "ax[2].axis(False)\n",
    "# ax[2].plot(370, 470, \"o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "el0mVSadUbMB",
    "outputId": "4c104e2d-88b0-4c21-ab8c-4d7f399ed460"
   },
   "outputs": [],
   "source": [
    "tt = np.array([1,2,3,4]).reshape(2,2)\n",
    "plt.imshow(tt, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPiYO5lq-T75"
   },
   "source": [
    "### Another implementation\n",
    "This is another implementation from pytorch, which is problematic since it only uses the first five layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hGccoo_PRYX7"
   },
   "outputs": [],
   "source": [
    "# desired depth layers to compute style/content losses :\n",
    "content_layers_default = ['conv_4']\n",
    "style_layers_default = ['conv_1', 'conv_2', 'conv_3', 'conv_4', 'conv_5']\n",
    "\n",
    "def get_style_model_and_losses(cnn, normalization_mean, normalization_std,\n",
    "                               style_img, content_img,\n",
    "                               content_layers=content_layers_default,\n",
    "                               style_layers=style_layers_default):\n",
    "    \"\"\"\n",
    "    A Sequential module contains an ordered list of child modules, including layers from VGG19\n",
    "    and some of new layers in order to extract and conpute the content and style loss.\"\"\"\n",
    "    cnn = copy.deepcopy(cnn)\n",
    "\n",
    "    # normalization module\n",
    "    normalization = Normalization(normalization_mean, normalization_std).to(device)\n",
    "\n",
    "    # just in order to have an iterable access to or list of content/syle\n",
    "    # losses\n",
    "    content_losses = []\n",
    "    style_losses = []\n",
    "\n",
    "    # assuming that cnn is a nn.Sequential, so we make a new nn.Sequential\n",
    "    # to put in modules that are supposed to be activated sequentially\n",
    "    model = nn.Sequential(normalization)\n",
    "\n",
    "    i = 0  # increment every time we see a conv\n",
    "    for layer in cnn.children():\n",
    "        if isinstance(layer, nn.Conv2d):\n",
    "            i += 1\n",
    "            name = 'conv_{}'.format(i)\n",
    "        elif isinstance(layer, nn.ReLU):\n",
    "            name = 'relu_{}'.format(i)\n",
    "            # The in-place version doesn't play very nicely with the ContentLoss\n",
    "            # and StyleLoss we insert below. So we replace with out-of-place\n",
    "            # ones here.\n",
    "            layer = nn.ReLU(inplace=False)\n",
    "        elif isinstance(layer, nn.MaxPool2d):\n",
    "            name = 'pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.AvgPool2d):\n",
    "            name = 'ave_pool_{}'.format(i)\n",
    "        elif isinstance(layer, nn.BatchNorm2d):\n",
    "            name = 'bn_{}'.format(i)\n",
    "        # else:\n",
    "        #     raise RuntimeError('Unrecognized layer: {}'.format(layer.__class__.__name__))\n",
    "\n",
    "        model.add_module(name, layer)\n",
    "\n",
    "        if name in content_layers:\n",
    "            # add content loss:\n",
    "            target = model(content_img).detach()\n",
    "            content_loss = ContentLoss(target)\n",
    "            model.add_module(\"content_loss_{}\".format(i), content_loss)\n",
    "            content_losses.append(content_loss)\n",
    "\n",
    "        if name in style_layers:\n",
    "            # add style loss:\n",
    "            target_feature = model(style_img).detach()\n",
    "            style_loss = StyleLoss(target_feature)\n",
    "            model.add_module(\"style_loss_{}\".format(i), style_loss)\n",
    "            style_losses.append(style_loss)\n",
    "\n",
    "    # now we trim off the layers after the last content and style losses\n",
    "    for i in range(len(model) - 1, -1, -1):\n",
    "        if isinstance(model[i], ContentLoss) or isinstance(model[i], StyleLoss):\n",
    "            break\n",
    "\n",
    "    model = model[:(i + 1)]\n",
    "\n",
    "    return model, style_losses, content_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KntdpMCA7AbI"
   },
   "outputs": [],
   "source": [
    "class ContentLoss(nn.Module):\n",
    "    def __init__(self, target,):\n",
    "        super(ContentLoss, self).__init__()\n",
    "        # we 'detach' the target content from the tree used to dynamically compute the gradient: \n",
    "        # this is a stated value, not a variable. Otherwise the forward method of the criterion\n",
    "        # will throw an error.\n",
    "        self.target = target.detach().clone()\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.loss = F.mse_loss(input, self.target)\n",
    "        # print(\"cont:\", self.loss)\n",
    "        return input\n",
    "\n",
    "def gram_matrix(input):\n",
    "    a, b, c, d = input.size()  # a=batch size(=1); b=# feature maps; (c,d)=dimensions of a f. map\n",
    "    features = input.view(b, c * d)  \n",
    "    G = torch.mm(features, features.t())  # compute the gram product\n",
    "\n",
    "    # we 'normalize' the values of the gram matrix by dividing by the number of element in each feature maps.\n",
    "    return G.div(a * b * c * d)\n",
    "\n",
    "class StyleLoss(nn.Module):\n",
    "    def __init__(self, target_feature):\n",
    "        super(StyleLoss, self).__init__()\n",
    "        self.target = gram_matrix(target_feature).detach().clone() # the Gram matrix of style image is fixed\n",
    "\n",
    "    def forward(self, input):\n",
    "        G = gram_matrix(input)\n",
    "        self.loss = F.mse_loss(G, self.target)\n",
    "        # print(\"style:\", self.loss)\n",
    "        return input\n",
    "     \n",
    "def run_style_transfer(cnn, normalization_mean, normalization_std,\n",
    "                       content_img, style_img, num_steps=300,\n",
    "                       style_weight=1000000, content_weight=1):\n",
    "    \"\"\"Run the style transfer.\"\"\"\n",
    "    input_img = initialize_img(content_img)\n",
    "    \n",
    "    print('Building the style transfer model..')\n",
    "    model, style_losses, content_losses = get_style_model_and_losses(cnn,\n",
    "        normalization_mean, normalization_std, style_img, content_img)\n",
    "    optimizer = get_input_optimizer(input_img)\n",
    "    display(model)\n",
    "\n",
    "    print('Optimizing..')\n",
    "    run = [0]\n",
    "    while run[0] <= num_steps:\n",
    "\n",
    "        def closure():\n",
    "            # correct the values of updated input image\n",
    "            input_img.data.clamp_(0, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            model(input_img)\n",
    "            style_score = 0\n",
    "            content_score = 0\n",
    "\n",
    "            for sl in style_losses:\n",
    "                style_score += sl.loss\n",
    "            for cl in content_losses:\n",
    "                content_score += cl.loss\n",
    "\n",
    "            style_score *= style_weight\n",
    "            content_score *= content_weight\n",
    "\n",
    "            loss = style_score + content_score\n",
    "            loss.backward()\n",
    "\n",
    "            run[0] += 1\n",
    "            if run[0] % 50 == 0:\n",
    "                print(\"run {}:\".format(run))\n",
    "                print('Style Loss : {:4f} Content Loss: {:4f}'.format(\n",
    "                    style_score.item(), content_score.item()))\n",
    "                print()\n",
    "\n",
    "            return style_score + content_score\n",
    "\n",
    "        optimizer.step(closure)\n",
    "\n",
    "    # a last correction...\n",
    "    input_img.data.clamp_(0, 1)\n",
    "\n",
    "    return input_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xhDpZoBvza7X",
    "outputId": "4b4cba22-bcc7-4f31-8347-a6ff0fe9b8fb"
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "style_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/Starry_Night.jpg\"\n",
    "content_img_path = \"/content/drive/MyDrive/Colab Notebooks/Neural_style_transfer/images/Tuebingen_Neckarfront.jpg\"\n",
    "content_img, style_img = load_images(content_img_path, style_img_path)\n",
    "output = run_style_transfer(cnn, cnn_normalization_mean, cnn_normalization_std,\n",
    "                            content_img, style_img, num_steps=300,\n",
    "                       style_weight=1e5, content_weight=1)\n",
    "\n",
    "print(\"Time usage: {:.4f}\".format(time.time() - start_time))\n",
    "imshow(output, title='Output Image', figsize=(10,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 483
    },
    "id": "8Eu6LlYe9pR2",
    "outputId": "355e2548-70c9-4899-c570-6fdb0f61dc76"
   },
   "outputs": [],
   "source": [
    "imshow(output, title='Output Image', figsize=(10,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QCX4MiSjF6s4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN7UicJKlbbXuU/nKgr6wHk",
   "collapsed_sections": [],
   "include_colab_link": true,
   "mount_file_id": "1WY_r76prvS1486gGr3R6cXkyv8NMPiLK",
   "name": "NST.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
